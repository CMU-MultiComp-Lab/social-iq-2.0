<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Social-IQ 2.0 Challenge @ ICCV 2023</title>
    <meta name="author" content="  ">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/social-iq-2.0/assets/img/siq_logo.png">
    
    <link rel="stylesheet" href="/social-iq-2.0/assets/css/main.css">
    <link rel="canonical" href="https://cmu-multicomp-lab.github.io/social-iq-2.0/">

    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/social-iq-2.0/">about<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Social-IQ 2.0 Challenge @ ICCV 2023
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          

          <div class="clearfix">
            <!-- ![alt text main fig](/assets/img/siq_banner_img.png){: .banner-image}
A novel dataset to benchmark Social Intelligence. -->

<!-- ![alt text paris](/social-iq-2.0/assets/img/paris.jpg){: .banner-image} -->
<p><img src="/social-iq-2.0/assets/img/paris2.jpg" alt="alt text paris" class="banner-image">
<!-- # The Social-IQ 2.0 Challenge --></p>

<p>The <b><a href="https://cmu-multicomp-lab.github.io/social-iq-2.0/">Social-IQ 2.0 Challenge</a></b> is designed to benchmark recent AI technologies‚Äô skills to reason about social interactions, which is referred as Artificial Social Intelligence. This challenge will be part of ICCV 2023 Workshop on Artificial Social Intelligence. The Social-IQ 2.0 dataset contains over 1,000 videos, accompanied with 6,000+ multiple-choice questions. The challenge plans to award over $1,200 in total prizes, and will be co-hosted with the <a href="https://sites.google.com/view/asi-iccv-2023/home" rel="external nofollow noopener" target="_blank">Artificial Social Intelligence Workshop</a> at <a href="https://iccv2023.thecvf.com" rel="external nofollow noopener" target="_blank">ICCV ‚Äò23</a>.</p>

<p><strong>Challenge pre-registration</strong>: Potential participants are asked to fill this simple online <a href="https://forms.gle/ZVTAvNunBQUa9ncJ6" rel="external nofollow noopener" target="_blank">form</a> with your email address so that we can keep you informed of relevant updates about the challenge.</p>

<h2 id="important-dates-">Important Dates üìÖ</h2>
<ul class="task-list">
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Challenge is released: <b>May 15</b>
</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Challenge and paper submission sites open, test set released: <del>mid-June</del> <strong>early-July</strong>
</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Paper submissions and final challenge submissions due: <b>July 21</b>
</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Acceptance decisions issued: <b>August 4</b>
</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>Camera ready paper due: <b>August 11</b>
</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled>ICCV workshop: <b>October 2</b>
</li>
</ul>

<h2 id="awards--and-prizes-">Awards üèÜ and Prizes üí∞</h2>
<p>There will be over $1,200 in prizes. We will give the following awards and may create additional awards as well.</p>
<ul>
  <li>
<strong>Challenge Winner</strong>: highest-performing submission</li>
  <li>
<strong>Best Few-Shot Paper</strong>: best paper in the Few-Shot Research Focus (see description below)</li>
  <li>
<strong>Best Fusion and Reasoning Paper</strong>: best paper in the Fusion and Reasoning Focus (see description below)</li>
</ul>

<h2 id="research-focuses-">Research Focuses üîç</h2>
<p>The study of artificial social intelligence brings many technical challenges for computer vision, natural language processing and machine learning. In order to encourage diversity in the proposed approaches, two of the awards will be given to the following two categories: Few-shot Research Focus and Fusion and Reasoning Focus. We also invite submissions in a third category: Representation Learning Focus. All three categories are described below.</p>

<h3 id="few-shot-focus-">Few-Shot Focus üéØ</h3>
<p>This research category is intended to encourage submission that are taking advantage of recent pretrained models and focus on composing these foundation models in a zero- or few-shot manner. One advantage of this research category is that it is potential accessible to a broader community, even with limited computations. As pre-trained foundation models will improve, it is an important research question to evaluate and benchmark their skills in understanding social interactions. Researchers can even decide to focus only on language as an intermediate representation, such as proposed in the <a href="https://socraticmodels.github.io/" rel="external nofollow noopener" target="_blank">Socratic Models</a>. It is ok to use API-accessed models such as ChatGPT and Bard.</p>

<p>Papers in this category should either use none of the training samples from Social-IQ 2.0 dataset (zero-shot learning) or very few of them, usually below 10 samples (few-shot learning).</p>

<h3 id="fusion-and-reasoning-focus-">Fusion and Reasoning Focus üß†</h3>
<p>This category is designed to foster research that explore new techniques to fuse multimodal information. Research papers in this category should focus on the fusion and reasoning parts of the algorithms, instead to the representation learning part (see the 3rd paper category below for description of representation learning focus). Since the focus is on the fusion and not about training new feature representation models (e.g., fine-tuning a pre-trained model), we expect models trained in this category to have fewer parameters to train and instead focus on how to best fuse multimodal information and reason over the observed premises. The reasoning can happen over multiple events (e.g., each speaking turn) present in the video. An inspiration in this category is the family of models that grew from the <a href="https://arxiv.org/pdf/1907.03950.pdf" rel="external nofollow noopener" target="_blank">Neural State Machine</a>.</p>

<p>Papers in this category should not try to fine-tune the pretrained models used for feature extraction and instead focus on learning a new model to fuse and reason over these multimodal features. Social-IQ 2.0 training dataset will be used to learning this fusion and reasoning module.</p>

<h3 id="representation-learning-focus-Ô∏è">Representation Learning Focus üéõÔ∏è</h3>
<p>We added this third category to acknowledge all the impactful research happening in multimodal representation learning. We acknowledge that not all researchers will have the computation power to train or fine-tune these large models, and this is why no direct award will be given to this category, beyond the Challenge Winner award. But we still believe that it is important to mention this research category. For this category, you may use as much compute as you would like, provided (as with all challenge submissions) that you only use data in the Social-IQ 2.0 dataset release for your training. Please reach out to us if you intend to use specialized hardware such as TPU‚Äôs, so we can ensure your code is reproducible. If you would like to use TPU‚Äôs, we recommend applying for <a href="https://sites.research.google/trc/about/" rel="external nofollow noopener" target="_blank">this program from Google</a>; their generous support allowed us to run the MERLOT-Reserve baselines for this challenge. You may decide to explore self-supervised setting (where the labels are not used during the representation learning phase) or directly train your representation model in a supervised setting such as MERLOT‚Äôs finetuning results.</p>

<h2 id="paper-submissions-">Paper Submissions üì§</h2>
<p>For submissions to be considered for the Social-IQ 2.0 challenge, teams are required to submit a research paper describing the approach in detail and share a link to their github repository. This github link will enable the review committee to ensure that results are genuine and reproducible. During paper submission, please specify the research focus (out of the three categories previously mentioned) you would like your paper to be considered for. Please format your papers according to <a href="https://iccv2023.thecvf.com/submission.guidelines-361600-2-20-16.php" rel="external nofollow noopener" target="_blank">ICCV‚Äôs submission guidelines</a>. Papers can be up to 6 pages (with no cap on additional pages for references and appendices). You can submit through CMT at the link below:</p>

<p><a href="https://cmt3.research.microsoft.com/asisiqiccv2023" rel="external nofollow noopener" target="_blank"><strong>Paper and Challenge Submission Link</strong></a></p>

<h2 id="challenge-submissions-Ô∏è">Challenge Submissions üèîÔ∏è</h2>
<p>It is expected that teams will use the validation data split to evaluate their model performance. Once a promising approach is found, teams will submit their predictions on the test set (<code class="language-plaintext highlighter-rouge">siq2/qa/qa_test.json</code>) along with their paper pdf using the online submission site linked above. Performance on the test set will be used to decide the Challenge Winner. The <code class="language-plaintext highlighter-rouge">qa_test.json</code> file in this repository is a superset of the test set that will be used for scoring.</p>

<p>The <code class="language-plaintext highlighter-rouge">qa_test.json</code> file does not have labels, so please indicate your method‚Äôs predicted labels by adding a field ‚Äúanswer_idx‚Äù with an index between 0 and 3 for each question.</p>

<p>In essence, the current qa_test.json contains the following fields: [qid, q, vid_name, ts, a0, a1, a2, a3]. Your qa_test.json will contain: [qid, q, vid_name, ts, a0, a1, a2, a3, <strong>answer_idx</strong>], where answer_idx indicates which index your model believes is the answer for this question.</p>

<p><strong>Please do not alter the other field names, or include any other fields in your submission</strong>.</p>

<p>IMPORTANT: Please do not perform additional training or adaptation on the test set. The training should be oly happening on the training set of the Social-IQ 2.0 dataset.</p>

<h2 id="review-criteria-">Review Criteria üéâ</h2>
<p>The following four criteria will be taken into consideration during the review process:</p>
<ol>
  <li>The contribution / novelty of the approach</li>
  <li>The reproducibility of the results</li>
  <li>The clarity of the writing</li>
  <li>Experimental results and analysis</li>
</ol>

<p>The review process will also be taking into consideration the category specified for the research focus. Innovative approaches in few-shot or zero-shot settings are particularly interesting. Similarly for new techniques to perform fusion and reasoning. In addition, we believe that negative results also have a place in these competitions, as long as they are accompanied with insightful discussions, so future researchers can learn from your experience.</p>

<h2 id="the-social-iq-20-dataset">The Social-IQ 2.0 Dataset</h2>
<p>We provide scripts to download the videos and question / answer files of Social-IQ 2.0 dataset. The videos are from three different categories: general youtube videos containing socially rich situations, short clips from movies, and clips containing socially rich situations in passenger vehicles.</p>

<p>The <code class="language-plaintext highlighter-rouge">siq2</code> folder looks like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>siq2
‚îú‚îÄ‚îÄ audio
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ mp3 # will contain mp3 files
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ wav # will contain wav files
‚îú‚îÄ‚îÄ download_all.py
‚îú‚îÄ‚îÄ frames
‚îú‚îÄ‚îÄ original_split.json # contains which video ids are in train/val/test splits for the different subsets: youtubeclips, movieclips, car clips
‚îú‚îÄ‚îÄ qa # contains question / answer labelled data; will contain unlabelled qa_test.json file when submission site is released
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ qa_train.json
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ qa_val.json
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ transcript # will contain .vtt transcript files
‚îú‚îÄ‚îÄ trims.json # contains start times for the videos; videos are 60 seconds long from that start second
‚îî‚îÄ‚îÄ video # will contain .mp4 video files
‚îî‚îÄ‚îÄ youtube_utils.py
</code></pre></div></div>

<p>To download the data, first install all required dependencies:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create -n siq2 python=3.8 -y &amp;&amp; conda activate siq2
pip install -r requirements.txt
sudo apt update &amp;&amp; sudo apt install ffmpeg # or however you need to install ffmpeg; varies per machine
</code></pre></div></div>

<p>Then, run the following to download the dataset</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python siq2/download_all.py # this takes about 4 hours to download and 60GB to store the whole dataset. This goes down to 30GB if you run with flag --no_frames
</code></pre></div></div>
<p>This will update <code class="language-plaintext highlighter-rouge">siq2/current_split.json</code>, which will describe the videos in the train/val/test splits. There is also <code class="language-plaintext highlighter-rouge">siq2/original_split.json</code>, which contains all the video ID‚Äôs we have questions and answers for, regardless of whether they‚Äôre available at the moment on youtube or not.</p>

<p><strong>A finer point about the dataset</strong>: Because you will download videos from youtube directly, the set of videos that constitute the dataset may change slightly between the release and conclusion of the challenge. <strong>We will treat the ‚Äúfinal‚Äù dataset as the set of videos downloaded from youtube one week before the challenge concludes</strong>. If you download the dataset now and report your results later, that is ok ‚Äì¬†we will simply discard predictions made on test set videos that are no longer available when we determine final testing accuracies, and you will not be penalized for training or validating on videos that are no longer available by the end of the challenge.</p>

<h2 id="questions">Questions</h2>
<p>If you have any questions, please open a Github issue on this repository or email awilf@cs.cmu.edu</p>

<p>The <a href="https://cmu-multicomp-lab.github.io/social-iq-2.0/">Social-IQ 2.0 Challenge</a> was created by the <a href="http://multicomp.cs.cmu.edu" rel="external nofollow noopener" target="_blank">MultiComp Lab</a> at <a href="https://www.cmu.edu" rel="external nofollow noopener" target="_blank">CMU</a>.</p>

<h2 id="team">Team</h2>
<p>The <a href="https://cmu-multicomp-lab.github.io/social-iq-2.0/">Social-IQ 2.0 Challenge</a> was created by several members of the <a href="http://multicomp.cs.cmu.edu" rel="external nofollow noopener" target="_blank">MultiComp Lab</a> at <a href="https://www.cmu.edu" rel="external nofollow noopener" target="_blank">CMU</a>.</p>

<table align="center">
  <tr>
     <td>
        <img style="width: 150px" src="/social-iq-2.0/assets/img/alexwilf.jpg">
         <br><a href="https://abwilf.github.io" rel="external nofollow noopener" target="_blank">Alex Wilf</a>
     </td>
     <td>
        <img style="width: 150px" src="/social-iq-2.0/assets/img/leena.jpg">
         <br><a href="https://l-mathur.github.io" rel="external nofollow noopener" target="_blank">Leena Mathur</a>
     </td>
     <td>
        <img style="width: 150px" src="/social-iq-2.0/assets/img/youssouf.jpg">
         <br><a href="https://gkebe.github.io" rel="external nofollow noopener" target="_blank">Youssouf Kebe</a>
     </td>
    <td>
        <img style="width: 150px" src="/social-iq-2.0/assets/img/paul.jpeg">
         <br><a href="https://www.cs.cmu.edu/~pliang/" rel="external nofollow noopener" target="_blank">Paul Liang</a>
     </td>

  </tr>
     <td>
        <img style="width: 150px" src="/social-iq-2.0/assets/img/sheryl2.jpg">
         <br><a href="https://www.linkedin.com/in/sheryl-m-a26809188/" rel="external nofollow noopener" target="_blank">Sheryl Mathew</a>
     </td>
     <td>
        <img style="width: 150px" src="/social-iq-2.0/assets/img/claire.jpeg">
         <br><a href="https://www.linkedin.com/in/claire-ko-053706190/" rel="external nofollow noopener" target="_blank">Claire Ko</a>
     </td>
      <td>
        <img style="width: 150px" src="/social-iq-2.0/assets/img/lp.jpg">
         <br><a href="https://www.cs.cmu.edu/~morency/" rel="external nofollow noopener" target="_blank">Louis-Philippe Morency</a>
     </td>
</table>

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->


          </div>

          <!-- News -->
          

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          

          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%61%77%69%6C%66@%61%6E%64%72%65%77.%63%6D%75.%65%64%75" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://github.com/abwilf/Social-IQ-2.0-Challenge" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            

              </div>

              <div class="contact-note">
                Contact us at the email above, or open a Github issue!

              </div>

            </div>
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2023   . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/social-iq-2.0/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/social-iq-2.0/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/social-iq-2.0/assets/js/no_defer.js"></script>
  <script defer src="/social-iq-2.0/assets/js/common.js"></script>
  <script defer src="/social-iq-2.0/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
